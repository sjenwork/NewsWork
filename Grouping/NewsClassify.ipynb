{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"NewsClassify.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"XtrIjzAJJmOt"},"source":["!pip install transformers\n","from google.colab import drive\n","import os\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/My Drive/SimEnvi/Project/109E10_ChemCloud/For_deploy/NewsWork/Grouping')\n","# %load NewsClassify.py\n","import pandas as pd\n","import numpy as np\n","import sys, re, time\n","from tqdm import trange\n","from tqdm import tqdm\n","\n","import torch\n","from torch import cuda\n","from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n","from torch.nn import CrossEntropyLoss, MSELoss\n","from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\n","\n","from multiprocessing import Pool, cpu_count\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1BXx-lV6eCvp","executionInfo":{"status":"ok","timestamp":1602769104105,"user_tz":-480,"elapsed":69006,"user":{"displayName":"Jen Sun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6op6XzbWKSjdP2qFOJYWqSA8NHvUnkEkW0-kt=s64","userId":"15778833878543637969"}},"outputId":"38233016-2f49-427b-b357-c50e44fb1384","colab":{"base_uri":"https://localhost:8080/","height":343}},"source":["\n","def labelmap():\n","    return dict(zip(('毒品', '災害防治', '環境汙染', '食品安全', '其他', '無關') , [i for i in range(6) ]))\n","\n","def labelmapInv():\n","    return {j:i for i,j in labelmap().items()}\n","\n","def longcut(sentence, num=480):\n","    if type(sentence) != list:\n","        sentence = [sentence]\n","    def cut(string):\n","        head = string[:num]\n","        ind = list(re.compile('。|;|；|！|!|\\.').finditer(head))\n","        if ind != []:\n","            ind = ind[-1].start()+1\n","        else:\n","            ind= 100\n","        res = [string[:ind], string[ind:]]\n","        return res\n","    i = 0\n","    while len(sentence[-1]) > num:\n","        finish = sentence[:-1]\n","        unfini = sentence[-1]\n","        res = cut(unfini)\n","        sentence = finish + res\n","        i+=1\n","        if i >20:\n","          sentence[-1] = []\n","    return sentence\n","\n","class SettingPath():\n","    def __init__(self, TASK_NAME, workType):\n","        self.workType = workType\n","        self._path(TASK_NAME)\n","        self._name(workType)\n","\n","    def _path(self, TASK_NAME):\n","        self.BASE_PATH  = '/content/drive/My Drive/A21_NewsPrediction'\n","        self.TASK_NAME  = TASK_NAME\n","        self.DATA_DIR   = DATA_DIR    = 'DATA'\n","        self.OUTPUT_DIR = OUTPUT_DIR  = 'output_model'\n","\n","        self.OUTPUT_DIR_TASK = os.path.join(OUTPUT_DIR, f'{TASK_NAME}')\n","\n","\n","    def _name(self, workType):\n","        self.CONFIG_NAME = CONFIG_NAME = 'config.json'\n","        self.WEIGHT_NAME = WEIGHT_NAME = 'pytorch_model.bin'\n","        self.CORPUS_NAME = CORPUS_NAME = 'vocab.txt'\n","\n","class preProcessor(SettingPath):\n","    def __init__(self, procFname ,TASK_NAME, workType, selCols=['label', 'text'], max_seq_length=480, **kwargs):\n","        super(preProcessor, self).__init__(TASK_NAME, workType)\n","        self.max_seq_length = 480\n","        self.procInFile(procFname, selCols, workType)\n","\n","    def __read__(self, fname):\n","        FileForm = fname.split('.')[-1]\n","        print(f'    >> 讀取資料中：')\n","        print(f'     >>> 檔案路徑：{fname}')\n","        print(f'     >>> 檔案格式 is: {FileForm}')\n","        assert FileForm in ['xls', 'xlsx', 'csv'], f'bad file extension {FileForm}'\n","        read = {'xls' : pd.read_excel,\n","                'xlsx': pd.read_excel,\n","                'csv' : pd.read_csv}\n","        return read[FileForm](fname)\n","\n","    def procInFile(self, fname, selCols, workType):\n","        df = self.__read__(fname)\n","        self.df = df = df[selCols]\n","\n","        if workType in ['train', 'test']:\n","            print('    >> 請使用已經整理完成之訓練集資料')\n","            df.columns = ['label', 'text']\n","            df_cut = df\n","            df_cut.to_excel('DATA/train_set_useless.xlsx', index=False)\n","\n","        elif workType == 'pred':\n","            print('    >> 預測資料準備中')\n","            df.columns = ['text']\n","            df['label'] = '其他'\n","            df_cut = self.cutNews(df['text'])\n","            df_cut.to_excel('DATA/pred_set_cut.xlsx', index=False)\n","\n","        self.df_cut = df_cut\n","\n","    def cutNews(self, text): \n","        num_of_word_per_news = self.max_seq_length - 20\n","        print(f'     >>> 將待預測之新聞分段，每段字數小於{num_of_word_per_news}字')\n","        text = text.str.replace('\\s+',' ')\n","        text_cut = pd.DataFrame(text.apply(lambda i: longcut(i, num=num_of_word_per_news)).to_list())\n","        text_cut.columns = [f'text_{i}' for i in text_cut.columns]\n","        df = (pd.concat([self.df, text_cut], axis=1)\n","                .drop('text', axis=1)\n","                .reset_index()\n","                )\n","        \n","        df = (pd.wide_to_long(df, 'text', i=['index', 'label'], j='count', sep='_')\n","               .reset_index()\n","               .drop(['count'], axis=1)\n","               .dropna(subset=['text'])\n","            #    .drop_duplicates(subset=['text'])\n","               .reset_index(drop=True)\n","               )\n","\n","        print(f'     >>> 避免字數過少的新聞片段影響判斷結果，移除少於10字之內容')\n","        df = df[df.text.str.len()>10]\n","        return df\n","\n","class lineFeatures():\n","    def __init__(self, label, text):\n","        self.label = label\n","        self.text  = text\n","\n","class InputFeatures():\n","    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n","        self.input_ids   = input_ids\n","        self.input_mask  = input_mask\n","        self.segment_ids = segment_ids\n","        self.label_id    = label_id\n","\n","def convert2feature(row):\n","    #max_seq_length = 500\n","\n","    tokens      = [\"[CLS]\"] + tokenizer.tokenize(row.text) + [\"[SEP]\"]\n","    input_ids   = tokenizer.convert_tokens_to_ids(tokens)\n","    segment_ids = [0] * len(tokens)\n","    input_mask  = [1] * len(input_ids)\n","    padding     = [0] * (max_seq_length - len(input_ids))\n","    input_ids   += padding\n","    input_mask  += padding\n","    segment_ids += padding\n","    label_id = labelmap()[row.label]\n","\n","    assert len(input_ids)   == max_seq_length\n","    assert len(input_mask)  == max_seq_length\n","    assert len(segment_ids) == max_seq_length\n","\n","    return InputFeatures(input_ids, input_mask, segment_ids, label_id)\n","\n","class toDataLoader(preProcessor):\n","    def __init__(self, procFname, TASK_NAME, workType, selCols, max_seq_length=480, batch_size=5):\n","        super(toDataLoader, self).__init__(procFname, TASK_NAME, workType, selCols, max_seq_length)\n","        df_feature = self.toloader(self.df_cut, batch_size)\n","\n","    def toloader(self, df, batch_size):\n","        lines = [lineFeatures(i['label'], i['text']) for _, i in df.iterrows()]\n","   \n","        print('    >> 將新聞內容輸出成Bert要求之格式') \n","        global max_seq_length \n","        max_seq_length = self.max_seq_length\n","        with Pool(4) as p:\n","            loader = list(tqdm(p.map(convert2feature, lines), total=len(lines)))\n","\n","        all_input_ids   = torch.tensor([i.input_ids   for i in loader], dtype=torch.long)\n","        all_input_mask  = torch.tensor([i.input_mask  for i in loader], dtype=torch.long)\n","        all_segment_ids = torch.tensor([i.segment_ids for i in loader], dtype=torch.long)\n","        all_label_ids   = torch.tensor([i.label_id    for i in loader], dtype=torch.long)\n","        data            = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n","        \n","        if self.workType == 'train':\n","            '''\n","            此步驟在訓練資料時一定要做\n","            '''\n","            print(f'     >>> 訓練資料集處理，隨機挑選樣本做為訓練資料')\n","            data_sampler    = RandomSampler(data)\n","            dataloader      = DataLoader(data, sampler=data_sampler, batch_size=batch_size)\n","        else:\n","            dataloader      = DataLoader(data, batch_size=batch_size)\n","\n","        self.batch_size = batch_size\n","        self.nlines = len(lines)\n","        self.n_label = len(labelmap())\n","        self.dataloader = dataloader\n","\n","def get_predictions(model, dataloader, compute_acc=False):\n","    predictions = None\n","    correct = 0\n","    total = 0\n","      \n","    with torch.no_grad():\n","        for data in dataloader:\n","            if next(model.parameters()).is_cuda:\n","                data = [t.to(\"cuda:0\") for t in data if t is not None]\n","            \n","            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n","            outputs = model(input_ids=tokens_tensors, \n","                            token_type_ids=segments_tensors, \n","                            attention_mask=masks_tensors)\n","            \n","            logits = outputs[0]\n","            _, pred = torch.max(logits.data, 1)\n","            \n","            if compute_acc:\n","                labels = data[3]\n","                total += labels.size(0)\n","                correct += (pred == labels).sum().item()\n","                \n","            if predictions is None:\n","                predictions = pred\n","            else:\n","                predictions = torch.cat((predictions, pred))\n","    \n","    if compute_acc:\n","        acc = correct / total\n","        return predictions, acc\n","    return predictions\n","\n","class trainModel(toDataLoader):\n","    def __init__(self, model, n_epochs, inParameter):\n","        super(trainModel, self).__init__(**inParameter)\n","        self.training(model, n_epochs)\n","\n","    def training(self, model, n_epochs):\n","        device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        print(f'    >> 使用{device}計算')\n","        model.to(device)\n","        model.train()\n","        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n","        \n","        _, acc = get_predictions(model, self.dataloader, compute_acc=True)\n","        print('     >>> initial acc: %.3f' %(acc))\n","        for epoch in trange(n_epochs, desc=\"Epoch\"):\n","            running_loss = 0.0\n","            for step, data in enumerate(tqdm(self.dataloader, desc=\"Iteration\")):\n","\n","                tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n","                optimizer.zero_grad()\n","                \n","                outputs = model(input_ids=tokens_tensors, \n","                                token_type_ids=segments_tensors, \n","                                attention_mask=masks_tensors, \n","                                labels=labels)\n","\n","                loss = outputs[0]\n","                loss.backward()\n","                optimizer.step()\n","                running_loss += loss.item()\n","            _, acc = get_predictions(model, self.dataloader, compute_acc=True)\n","            print('[epoch %d] loss: %.3f, acc: %.3f' %(epoch + 1, running_loss, acc))\n","        print('    >> 訓練完成，儲存訓練完之模型')\n","        model.save_pretrained('output_model/trained_model')\n","        self.model = model\n","\n","class predict(toDataLoader):\n","    def __init__(self, procFname, TASK_NAME, workType, selCols, batch_size, max_seq_length=500):\n","        super(predict, self).__init__(procFname, TASK_NAME, workType, selCols, max_seq_length, batch_size)\n","        self.predict()\n","\n","    def predict(self):\n","        device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        model.to(device)\n","        print(f'    >> 使用{device}計算')\n","\n","        predictions = None\n","        correct = 0\n","        total = 0\n","\n","        with torch.no_grad():\n","            tmp = []\n","            probs = []\n","            preds = []\n","            for data in tqdm(self.dataloader, desc=\"Evaluating\"):\n","                if next(model.parameters()).is_cuda:\n","                    data = [t.to(\"cuda:0\") for t in data if t is not None]\n","\n","                tokens_tensors, segments_tensors, masks_tensors = data[:3]\n","                outputs = model(input_ids=tokens_tensors, \n","                                token_type_ids=segments_tensors, \n","                                attention_mask=masks_tensors)\n","                \n","                logits = outputs[0]\n","                m = torch.nn.Softmax(dim=1)\n","                prob, pred = torch.topk(m(logits), 1)\n","                probs+=prob.tolist()\n","                preds+=pred.tolist()\n","                    \n","                if predictions is None:\n","                    predictions = pred[0]\n","                else:\n","                    predictions = torch.cat((predictions, pred[0]))\n","        \n","        self.tmp = tmp\n","        self.preds = predictions\n","        self.probs = probs\n","        self.preds = preds\n","\n","class saveLabel2file(SettingPath):\n","    def __init__(self, preds, probs, procFname, TASK_NAME, workType, **kwargs):\n","        print(f'    ------------------------------------------------')\n","        super(saveLabel2file, self).__init__(TASK_NAME, workType)\n","        self.procFname = procFname\n","        self.df = df = self.read(procFname)\n","        print('    >> 讀取分段後之待預測新聞內容：DATA/pred_set_cut.xlsx')\n","        self.df_cut = df_cut = self.read('DATA/pred_set_cut.xlsx')\n","        self.preds = preds\n","        self.relabel(df, df_cut, preds, probs)\n","\n","    def __read__(self, fname):\n","        FileForm = fname.split('.')[-1]\n","        print(f'   >> 讀取原始待預測之新聞內容：{fname}')\n","        assert FileForm in ['xls', 'xlsx', 'csv'], f'bad file extension {FileForm}'\n","        read = {'xls' : pd.read_excel,\n","                'xlsx': pd.read_excel,\n","                'csv' : pd.read_csv}\n","        return read[FileForm](fname)\n","\n","    def read(self, fname):\n","        df = self.__read__(fname)\n","        return df\n","\n","    def relabel(self, df, df_cut, preds, probs):\n","        preds = pd.DataFrame(preds).replace(labelmapInv()).iloc[:,[0]]\n","        probs = pd.DataFrame(probs).iloc[:,[0]]\n","        preds.columns = ['preds']\n","        probs.columns = ['probs']\n","        df_cut = pd.concat([df_cut, preds, probs], axis=1)\n","        print('    >> 輸出預測完成之分段後新聞：DATA/Result_cut.xlsx')\n","        df_cut.to_excel('DATA/Result_cut.xlsx')\n","\n","        print('    >> 根據分段新聞之預測結果，決定新聞最終之分類')\n","        prob = df_cut.pivot_table(index='index', columns='preds', aggfunc='mean').fillna(0).droplevel(0, axis=1)\n","        df = pd.concat([self.df, prob], axis=1)\n","        self.df_cut = df_cut\n","        self.prob = prob\n","        self.df = df\n","\n","        dirname = os.path.dirname(self.procFname)\n","        #form    = self.procFname.split('.')[-1]\n","        outName = os.path.join(dirname, f'Result_merged.xlsx')\n","        print(f'    >> 輸出預測完成且整併完成之檔案：{outName}')\n","        df.to_excel(outName)\n","        print(f'     >>> 執行結果： {os.listdir(\"DATA/\")}')\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["    >> 讀取資料中：\n","     >>> 檔案路徑：DATA/pred_set.xlsx\n","     >>> 檔案格式 is: xlsx\n","    >> 預測資料準備中\n","     >>> 將待預測之新聞分段，每段字數小於460字\n","     >>> 避免字數過少的新聞片段影響判斷結果，移除少於10字之內容\n","    >> 將新聞內容輸出成Bert要求之格式\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1085/1085 [00:00<00:00, 319333.37it/s]\n","Evaluating:   0%|          | 0/109 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["    >> 使用cuda計算\n"],"name":"stdout"},{"output_type":"stream","text":["Evaluating: 100%|██████████| 109/109 [00:32<00:00,  3.30it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["    ------------------------------------------------\n","   >> 讀取原始待預測之新聞內容：DATA/pred_set.xlsx\n","    >> 讀取分段後之待預測新聞內容：DATA/pred_set_cut.xlsx\n","   >> 讀取原始待預測之新聞內容：DATA/pred_set_cut.xlsx\n","    >> 輸出預測完成之分段後新聞：DATA/Result_cut.xlsx\n","    >> 根據分段新聞之預測結果，決定新聞最終之分類\n","    >> 輸出預測完成且整併完成之檔案：DATA/Result_merged.xlsx\n","     >>> 執行結果： ['train_set_v0.xlsx', 'train_set_useless.xlsx', 'trans.py', 'train_set.xlsx', 'pred_set.xlsx', 'pred_set_cut.xlsx', 'Result_cut.xlsx', 'Result_merged.xlsx', 'Result_aboard_merged.xlsx', 'pred_set_aboard.xlsx']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X88nc9BaetiU","executionInfo":{"status":"ok","timestamp":1602769684356,"user_tz":-480,"elapsed":295873,"user":{"displayName":"Jen Sun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6op6XzbWKSjdP2qFOJYWqSA8NHvUnkEkW0-kt=s64","userId":"15778833878543637969"}},"outputId":"d3ebe921-6f9f-4892-b225-cb66659fd67a","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["\n","if __name__ == '__main__':\n","    do_train = True\n","    do_pred = False\n","\n","    if do_pred:\n","        nlabels   = len(labelmap())\n","        TASK_NAME = 'news_classification'\n","        BERT_MODEL = 'bert-base-chinese'\n","        \n","        \n","        parameter = { 'TASK_NAME'  : TASK_NAME, \n","                      'procFname'  : 'DATA/pred_set.xlsx',\n","#                      'selCols'    : ['內文'],\n","                      'selCols'    : ['news_content'],\n","                      'workType'   : 'pred',\n","                      'max_seq_length' : 500,\n","                      'batch_size' : 10,\n","                     }\n","\n","        BERT_MODEL_trained  = os.path.join('output_model', f'trained_model')\n","        # BERT_MODEL          = 'bert-base-chinese'\n","        tokenizer           = BertTokenizer.from_pretrained(BERT_MODEL_trained)\n","        model               = BertForSequenceClassification.from_pretrained(BERT_MODEL_trained, num_labels=nlabels)\n","        p                   = predict(**parameter)\n","        s                   = saveLabel2file(p.preds, p.probs, **parameter)\n","\n","    if do_train:\n","        os.chdir('/content/drive/My Drive/SimEnvi/Project/109E10_ChemCloud/For_deploy/NewsWork/Grouping')\n","        nlabels   = len(labelmap())\n","        TASK_NAME = 'news_classification'\n","        BERT_MODEL = 'bert-base-chinese'\n","\n","        tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n","        model     = BertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels=nlabels)\n","        \n","        device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        model.to(device)\n","        parameter = { 'TASK_NAME'  : TASK_NAME, \n","                      'procFname'  : 'DATA/train_set.xlsx',\n","                      'selCols'    : ['label', 'text'],\n","                      'workType'   : 'train',\n","                      'batch_size' : 10,\n","                     }\n","        m = trainModel(model, 6, parameter)\n","\n","        # for test\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["    >> 讀取資料中：\n","     >>> 檔案路徑：DATA/train_set.xlsx\n","     >>> 檔案格式 is: xlsx\n","    >> 請使用已經整理完成之訓練集資料\n","    >> 將新聞內容輸出成Bert要求之格式\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 352/352 [00:00<00:00, 270649.86it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["     >>> 訓練資料集處理，隨機挑選樣本做為訓練資料\n","    >> 使用cuda計算\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch:   0%|          | 0/6 [00:00<?, ?it/s]\n","Iteration:   0%|          | 0/36 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["     >>> initial acc: 0.185\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Iteration:   3%|▎         | 1/36 [00:00<00:32,  1.07it/s]\u001b[A\n","Iteration:   6%|▌         | 2/36 [00:01<00:31,  1.07it/s]\u001b[A\n","Iteration:   8%|▊         | 3/36 [00:02<00:30,  1.08it/s]\u001b[A\n","Iteration:  11%|█         | 4/36 [00:03<00:29,  1.08it/s]\u001b[A\n","Iteration:  14%|█▍        | 5/36 [00:04<00:28,  1.07it/s]\u001b[A\n","Iteration:  17%|█▋        | 6/36 [00:05<00:27,  1.07it/s]\u001b[A\n","Iteration:  19%|█▉        | 7/36 [00:06<00:26,  1.07it/s]\u001b[A\n","Iteration:  22%|██▏       | 8/36 [00:07<00:26,  1.07it/s]\u001b[A\n","Iteration:  25%|██▌       | 9/36 [00:08<00:25,  1.07it/s]\u001b[A\n","Iteration:  28%|██▊       | 10/36 [00:09<00:24,  1.07it/s]\u001b[A\n","Iteration:  31%|███       | 11/36 [00:10<00:23,  1.07it/s]\u001b[A\n","Iteration:  33%|███▎      | 12/36 [00:11<00:22,  1.07it/s]\u001b[A\n","Iteration:  36%|███▌      | 13/36 [00:12<00:21,  1.07it/s]\u001b[A\n","Iteration:  39%|███▉      | 14/36 [00:13<00:20,  1.07it/s]\u001b[A\n","Iteration:  42%|████▏     | 15/36 [00:14<00:19,  1.07it/s]\u001b[A\n","Iteration:  44%|████▍     | 16/36 [00:14<00:18,  1.06it/s]\u001b[A\n","Iteration:  47%|████▋     | 17/36 [00:15<00:17,  1.06it/s]\u001b[A\n","Iteration:  50%|█████     | 18/36 [00:16<00:16,  1.06it/s]\u001b[A\n","Iteration:  53%|█████▎    | 19/36 [00:17<00:16,  1.06it/s]\u001b[A\n","Iteration:  56%|█████▌    | 20/36 [00:18<00:15,  1.06it/s]\u001b[A\n","Iteration:  58%|█████▊    | 21/36 [00:19<00:14,  1.06it/s]\u001b[A\n","Iteration:  61%|██████    | 22/36 [00:20<00:13,  1.05it/s]\u001b[A\n","Iteration:  64%|██████▍   | 23/36 [00:21<00:12,  1.05it/s]\u001b[A\n","Iteration:  67%|██████▋   | 24/36 [00:22<00:11,  1.05it/s]\u001b[A\n","Iteration:  69%|██████▉   | 25/36 [00:23<00:10,  1.05it/s]\u001b[A\n","Iteration:  72%|███████▏  | 26/36 [00:24<00:09,  1.05it/s]\u001b[A\n","Iteration:  75%|███████▌  | 27/36 [00:25<00:08,  1.05it/s]\u001b[A\n","Iteration:  78%|███████▊  | 28/36 [00:26<00:07,  1.04it/s]\u001b[A\n","Iteration:  81%|████████  | 29/36 [00:27<00:06,  1.04it/s]\u001b[A\n","Iteration:  83%|████████▎ | 30/36 [00:28<00:05,  1.04it/s]\u001b[A\n","Iteration:  86%|████████▌ | 31/36 [00:29<00:04,  1.04it/s]\u001b[A\n","Iteration:  89%|████████▉ | 32/36 [00:30<00:03,  1.04it/s]\u001b[A\n","Iteration:  92%|█████████▏| 33/36 [00:31<00:02,  1.04it/s]\u001b[A\n","Iteration:  94%|█████████▍| 34/36 [00:32<00:01,  1.04it/s]\u001b[A\n","Iteration:  97%|█████████▋| 35/36 [00:33<00:00,  1.04it/s]\u001b[A\n","Iteration: 100%|██████████| 36/36 [00:33<00:00,  1.08it/s]\n","Epoch:  17%|█▋        | 1/6 [00:45<03:47, 45.56s/it]\n","Iteration:   0%|          | 0/36 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["[epoch 1] loss: 60.103, acc: 0.616\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Iteration:   3%|▎         | 1/36 [00:00<00:34,  1.02it/s]\u001b[A\n","Iteration:   6%|▌         | 2/36 [00:01<00:33,  1.01it/s]\u001b[A\n","Iteration:   8%|▊         | 3/36 [00:02<00:32,  1.01it/s]\u001b[A\n","Iteration:  11%|█         | 4/36 [00:03<00:31,  1.01it/s]\u001b[A\n","Iteration:  14%|█▍        | 5/36 [00:04<00:30,  1.01it/s]\u001b[A\n","Iteration:  17%|█▋        | 6/36 [00:05<00:29,  1.02it/s]\u001b[A\n","Iteration:  19%|█▉        | 7/36 [00:06<00:28,  1.02it/s]\u001b[A\n","Iteration:  22%|██▏       | 8/36 [00:07<00:27,  1.02it/s]\u001b[A\n","Iteration:  25%|██▌       | 9/36 [00:08<00:26,  1.02it/s]\u001b[A\n","Iteration:  28%|██▊       | 10/36 [00:09<00:25,  1.02it/s]\u001b[A\n","Iteration:  31%|███       | 11/36 [00:10<00:24,  1.02it/s]\u001b[A\n","Iteration:  33%|███▎      | 12/36 [00:11<00:23,  1.02it/s]\u001b[A\n","Iteration:  36%|███▌      | 13/36 [00:12<00:22,  1.02it/s]\u001b[A\n","Iteration:  39%|███▉      | 14/36 [00:13<00:21,  1.02it/s]\u001b[A\n","Iteration:  42%|████▏     | 15/36 [00:14<00:20,  1.03it/s]\u001b[A\n","Iteration:  44%|████▍     | 16/36 [00:15<00:19,  1.03it/s]\u001b[A\n","Iteration:  47%|████▋     | 17/36 [00:16<00:18,  1.03it/s]\u001b[A\n","Iteration:  50%|█████     | 18/36 [00:17<00:17,  1.03it/s]\u001b[A\n","Iteration:  53%|█████▎    | 19/36 [00:18<00:16,  1.03it/s]\u001b[A\n","Iteration:  56%|█████▌    | 20/36 [00:19<00:15,  1.03it/s]\u001b[A\n","Iteration:  58%|█████▊    | 21/36 [00:20<00:14,  1.04it/s]\u001b[A\n","Iteration:  61%|██████    | 22/36 [00:21<00:13,  1.04it/s]\u001b[A\n","Iteration:  64%|██████▍   | 23/36 [00:22<00:12,  1.04it/s]\u001b[A\n","Iteration:  67%|██████▋   | 24/36 [00:23<00:11,  1.04it/s]\u001b[A\n","Iteration:  69%|██████▉   | 25/36 [00:24<00:10,  1.04it/s]\u001b[A\n","Iteration:  72%|███████▏  | 26/36 [00:25<00:09,  1.04it/s]\u001b[A\n","Iteration:  75%|███████▌  | 27/36 [00:26<00:08,  1.04it/s]\u001b[A\n","Iteration:  78%|███████▊  | 28/36 [00:27<00:07,  1.04it/s]\u001b[A\n","Iteration:  81%|████████  | 29/36 [00:28<00:06,  1.04it/s]\u001b[A\n","Iteration:  83%|████████▎ | 30/36 [00:29<00:05,  1.04it/s]\u001b[A\n","Iteration:  86%|████████▌ | 31/36 [00:30<00:04,  1.05it/s]\u001b[A\n","Iteration:  89%|████████▉ | 32/36 [00:31<00:03,  1.05it/s]\u001b[A\n","Iteration:  92%|█████████▏| 33/36 [00:32<00:02,  1.05it/s]\u001b[A\n","Iteration:  94%|█████████▍| 34/36 [00:32<00:01,  1.05it/s]\u001b[A\n","Iteration:  97%|█████████▋| 35/36 [00:33<00:00,  1.05it/s]\u001b[A\n","Iteration: 100%|██████████| 36/36 [00:34<00:00,  1.05it/s]\n","Epoch:  33%|███▎      | 2/6 [01:31<03:02, 45.67s/it]\n","Iteration:   0%|          | 0/36 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["[epoch 2] loss: 41.722, acc: 0.841\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Iteration:   3%|▎         | 1/36 [00:00<00:33,  1.05it/s]\u001b[A\n","Iteration:   6%|▌         | 2/36 [00:01<00:32,  1.05it/s]\u001b[A\n","Iteration:   8%|▊         | 3/36 [00:02<00:31,  1.05it/s]\u001b[A\n","Iteration:  11%|█         | 4/36 [00:03<00:30,  1.05it/s]\u001b[A\n","Iteration:  14%|█▍        | 5/36 [00:04<00:29,  1.05it/s]\u001b[A\n","Iteration:  17%|█▋        | 6/36 [00:05<00:28,  1.05it/s]\u001b[A\n","Iteration:  19%|█▉        | 7/36 [00:06<00:27,  1.05it/s]\u001b[A\n","Iteration:  22%|██▏       | 8/36 [00:07<00:26,  1.05it/s]\u001b[A\n","Iteration:  25%|██▌       | 9/36 [00:08<00:25,  1.04it/s]\u001b[A\n","Iteration:  28%|██▊       | 10/36 [00:09<00:24,  1.04it/s]\u001b[A\n","Iteration:  31%|███       | 11/36 [00:10<00:23,  1.04it/s]\u001b[A\n","Iteration:  33%|███▎      | 12/36 [00:11<00:22,  1.04it/s]\u001b[A\n","Iteration:  36%|███▌      | 13/36 [00:12<00:22,  1.05it/s]\u001b[A\n","Iteration:  39%|███▉      | 14/36 [00:13<00:21,  1.04it/s]\u001b[A\n","Iteration:  42%|████▏     | 15/36 [00:14<00:20,  1.04it/s]\u001b[A\n","Iteration:  44%|████▍     | 16/36 [00:15<00:19,  1.04it/s]\u001b[A\n","Iteration:  47%|████▋     | 17/36 [00:16<00:18,  1.04it/s]\u001b[A\n","Iteration:  50%|█████     | 18/36 [00:17<00:17,  1.04it/s]\u001b[A\n","Iteration:  53%|█████▎    | 19/36 [00:18<00:16,  1.04it/s]\u001b[A\n","Iteration:  56%|█████▌    | 20/36 [00:19<00:15,  1.04it/s]\u001b[A\n","Iteration:  58%|█████▊    | 21/36 [00:20<00:14,  1.04it/s]\u001b[A\n","Iteration:  61%|██████    | 22/36 [00:21<00:13,  1.04it/s]\u001b[A\n","Iteration:  64%|██████▍   | 23/36 [00:22<00:12,  1.04it/s]\u001b[A\n","Iteration:  67%|██████▋   | 24/36 [00:23<00:11,  1.04it/s]\u001b[A\n","Iteration:  69%|██████▉   | 25/36 [00:23<00:10,  1.04it/s]\u001b[A\n","Iteration:  72%|███████▏  | 26/36 [00:24<00:09,  1.04it/s]\u001b[A\n","Iteration:  75%|███████▌  | 27/36 [00:25<00:08,  1.04it/s]\u001b[A\n","Iteration:  78%|███████▊  | 28/36 [00:26<00:07,  1.04it/s]\u001b[A\n","Iteration:  81%|████████  | 29/36 [00:27<00:06,  1.04it/s]\u001b[A\n","Iteration:  83%|████████▎ | 30/36 [00:28<00:05,  1.03it/s]\u001b[A\n","Iteration:  86%|████████▌ | 31/36 [00:29<00:04,  1.03it/s]\u001b[A\n","Iteration:  89%|████████▉ | 32/36 [00:30<00:03,  1.03it/s]\u001b[A\n","Iteration:  92%|█████████▏| 33/36 [00:31<00:02,  1.04it/s]\u001b[A\n","Iteration:  94%|█████████▍| 34/36 [00:32<00:01,  1.03it/s]\u001b[A\n","Iteration:  97%|█████████▋| 35/36 [00:33<00:00,  1.03it/s]\u001b[A\n","Iteration: 100%|██████████| 36/36 [00:33<00:00,  1.06it/s]\n","Epoch:  50%|█████     | 3/6 [02:17<02:17, 45.75s/it]\n","Iteration:   0%|          | 0/36 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["[epoch 3] loss: 25.510, acc: 0.906\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Iteration:   3%|▎         | 1/36 [00:00<00:33,  1.03it/s]\u001b[A\n","Iteration:   6%|▌         | 2/36 [00:01<00:32,  1.03it/s]\u001b[A\n","Iteration:   8%|▊         | 3/36 [00:02<00:32,  1.03it/s]\u001b[A\n","Iteration:  11%|█         | 4/36 [00:03<00:31,  1.03it/s]\u001b[A\n","Iteration:  14%|█▍        | 5/36 [00:04<00:29,  1.03it/s]\u001b[A\n","Iteration:  17%|█▋        | 6/36 [00:05<00:29,  1.03it/s]\u001b[A\n","Iteration:  19%|█▉        | 7/36 [00:06<00:28,  1.03it/s]\u001b[A\n","Iteration:  22%|██▏       | 8/36 [00:07<00:27,  1.04it/s]\u001b[A\n","Iteration:  25%|██▌       | 9/36 [00:08<00:26,  1.04it/s]\u001b[A\n","Iteration:  28%|██▊       | 10/36 [00:09<00:25,  1.04it/s]\u001b[A\n","Iteration:  31%|███       | 11/36 [00:10<00:24,  1.03it/s]\u001b[A\n","Iteration:  33%|███▎      | 12/36 [00:11<00:23,  1.03it/s]\u001b[A\n","Iteration:  36%|███▌      | 13/36 [00:12<00:22,  1.04it/s]\u001b[A\n","Iteration:  39%|███▉      | 14/36 [00:13<00:21,  1.04it/s]\u001b[A\n","Iteration:  42%|████▏     | 15/36 [00:14<00:20,  1.04it/s]\u001b[A\n","Iteration:  44%|████▍     | 16/36 [00:15<00:19,  1.04it/s]\u001b[A\n","Iteration:  47%|████▋     | 17/36 [00:16<00:18,  1.04it/s]\u001b[A\n","Iteration:  50%|█████     | 18/36 [00:17<00:17,  1.03it/s]\u001b[A\n","Iteration:  53%|█████▎    | 19/36 [00:18<00:16,  1.03it/s]\u001b[A\n","Iteration:  56%|█████▌    | 20/36 [00:19<00:15,  1.03it/s]\u001b[A\n","Iteration:  58%|█████▊    | 21/36 [00:20<00:14,  1.04it/s]\u001b[A\n","Iteration:  61%|██████    | 22/36 [00:21<00:13,  1.04it/s]\u001b[A\n","Iteration:  64%|██████▍   | 23/36 [00:22<00:12,  1.04it/s]\u001b[A\n","Iteration:  67%|██████▋   | 24/36 [00:23<00:11,  1.04it/s]\u001b[A\n","Iteration:  69%|██████▉   | 25/36 [00:24<00:10,  1.04it/s]\u001b[A\n","Iteration:  72%|███████▏  | 26/36 [00:25<00:09,  1.04it/s]\u001b[A\n","Iteration:  75%|███████▌  | 27/36 [00:26<00:08,  1.04it/s]\u001b[A\n","Iteration:  78%|███████▊  | 28/36 [00:27<00:07,  1.04it/s]\u001b[A\n","Iteration:  81%|████████  | 29/36 [00:27<00:06,  1.04it/s]\u001b[A\n","Iteration:  83%|████████▎ | 30/36 [00:28<00:05,  1.04it/s]\u001b[A\n","Iteration:  86%|████████▌ | 31/36 [00:29<00:04,  1.04it/s]\u001b[A\n","Iteration:  89%|████████▉ | 32/36 [00:30<00:03,  1.04it/s]\u001b[A\n","Iteration:  92%|█████████▏| 33/36 [00:31<00:02,  1.04it/s]\u001b[A\n","Iteration:  94%|█████████▍| 34/36 [00:32<00:01,  1.04it/s]\u001b[A\n","Iteration:  97%|█████████▋| 35/36 [00:33<00:00,  1.04it/s]\u001b[A\n","Iteration: 100%|██████████| 36/36 [00:34<00:00,  1.06it/s]\n","Epoch:  67%|██████▋   | 4/6 [03:03<01:31, 45.81s/it]\n","Iteration:   0%|          | 0/36 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["[epoch 4] loss: 14.905, acc: 0.957\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Iteration:   3%|▎         | 1/36 [00:00<00:33,  1.04it/s]\u001b[A\n","Iteration:   6%|▌         | 2/36 [00:01<00:32,  1.04it/s]\u001b[A\n","Iteration:   8%|▊         | 3/36 [00:02<00:31,  1.04it/s]\u001b[A\n","Iteration:  11%|█         | 4/36 [00:03<00:30,  1.04it/s]\u001b[A\n","Iteration:  14%|█▍        | 5/36 [00:04<00:29,  1.04it/s]\u001b[A\n","Iteration:  17%|█▋        | 6/36 [00:05<00:28,  1.04it/s]\u001b[A\n","Iteration:  19%|█▉        | 7/36 [00:06<00:27,  1.04it/s]\u001b[A\n","Iteration:  22%|██▏       | 8/36 [00:07<00:27,  1.04it/s]\u001b[A\n","Iteration:  25%|██▌       | 9/36 [00:08<00:26,  1.04it/s]\u001b[A\n","Iteration:  28%|██▊       | 10/36 [00:09<00:25,  1.04it/s]\u001b[A\n","Iteration:  31%|███       | 11/36 [00:10<00:24,  1.04it/s]\u001b[A\n","Iteration:  33%|███▎      | 12/36 [00:11<00:23,  1.03it/s]\u001b[A\n","Iteration:  36%|███▌      | 13/36 [00:12<00:22,  1.04it/s]\u001b[A\n","Iteration:  39%|███▉      | 14/36 [00:13<00:21,  1.04it/s]\u001b[A\n","Iteration:  42%|████▏     | 15/36 [00:14<00:20,  1.04it/s]\u001b[A\n","Iteration:  44%|████▍     | 16/36 [00:15<00:19,  1.04it/s]\u001b[A\n","Iteration:  47%|████▋     | 17/36 [00:16<00:18,  1.04it/s]\u001b[A\n","Iteration:  50%|█████     | 18/36 [00:17<00:17,  1.04it/s]\u001b[A\n","Iteration:  53%|█████▎    | 19/36 [00:18<00:16,  1.04it/s]\u001b[A\n","Iteration:  56%|█████▌    | 20/36 [00:19<00:15,  1.04it/s]\u001b[A\n","Iteration:  58%|█████▊    | 21/36 [00:20<00:14,  1.04it/s]\u001b[A\n","Iteration:  61%|██████    | 22/36 [00:21<00:13,  1.04it/s]\u001b[A\n","Iteration:  64%|██████▍   | 23/36 [00:22<00:12,  1.04it/s]\u001b[A\n","Iteration:  67%|██████▋   | 24/36 [00:23<00:11,  1.04it/s]\u001b[A\n","Iteration:  69%|██████▉   | 25/36 [00:24<00:10,  1.03it/s]\u001b[A\n","Iteration:  72%|███████▏  | 26/36 [00:25<00:09,  1.03it/s]\u001b[A\n","Iteration:  75%|███████▌  | 27/36 [00:26<00:08,  1.03it/s]\u001b[A\n","Iteration:  78%|███████▊  | 28/36 [00:27<00:07,  1.03it/s]\u001b[A\n","Iteration:  81%|████████  | 29/36 [00:27<00:06,  1.03it/s]\u001b[A\n","Iteration:  83%|████████▎ | 30/36 [00:28<00:05,  1.04it/s]\u001b[A\n","Iteration:  86%|████████▌ | 31/36 [00:29<00:04,  1.04it/s]\u001b[A\n","Iteration:  89%|████████▉ | 32/36 [00:30<00:03,  1.04it/s]\u001b[A\n","Iteration:  92%|█████████▏| 33/36 [00:31<00:02,  1.04it/s]\u001b[A\n","Iteration:  94%|█████████▍| 34/36 [00:32<00:01,  1.04it/s]\u001b[A\n","Iteration:  97%|█████████▋| 35/36 [00:33<00:00,  1.04it/s]\u001b[A\n","Iteration: 100%|██████████| 36/36 [00:34<00:00,  1.06it/s]\n","Epoch:  83%|████████▎ | 5/6 [03:49<00:45, 45.86s/it]\n","Iteration:   0%|          | 0/36 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["[epoch 5] loss: 9.388, acc: 0.994\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Iteration:   3%|▎         | 1/36 [00:00<00:33,  1.04it/s]\u001b[A\n","Iteration:   6%|▌         | 2/36 [00:01<00:32,  1.03it/s]\u001b[A\n","Iteration:   8%|▊         | 3/36 [00:02<00:31,  1.03it/s]\u001b[A\n","Iteration:  11%|█         | 4/36 [00:03<00:30,  1.03it/s]\u001b[A\n","Iteration:  14%|█▍        | 5/36 [00:04<00:30,  1.03it/s]\u001b[A\n","Iteration:  17%|█▋        | 6/36 [00:05<00:29,  1.03it/s]\u001b[A\n","Iteration:  19%|█▉        | 7/36 [00:06<00:28,  1.03it/s]\u001b[A\n","Iteration:  22%|██▏       | 8/36 [00:07<00:27,  1.03it/s]\u001b[A\n","Iteration:  25%|██▌       | 9/36 [00:08<00:26,  1.03it/s]\u001b[A\n","Iteration:  28%|██▊       | 10/36 [00:09<00:25,  1.03it/s]\u001b[A\n","Iteration:  31%|███       | 11/36 [00:10<00:24,  1.03it/s]\u001b[A\n","Iteration:  33%|███▎      | 12/36 [00:11<00:23,  1.03it/s]\u001b[A\n","Iteration:  36%|███▌      | 13/36 [00:12<00:22,  1.03it/s]\u001b[A\n","Iteration:  39%|███▉      | 14/36 [00:13<00:21,  1.03it/s]\u001b[A\n","Iteration:  42%|████▏     | 15/36 [00:14<00:20,  1.03it/s]\u001b[A\n","Iteration:  44%|████▍     | 16/36 [00:15<00:19,  1.03it/s]\u001b[A\n","Iteration:  47%|████▋     | 17/36 [00:16<00:18,  1.03it/s]\u001b[A\n","Iteration:  50%|█████     | 18/36 [00:17<00:17,  1.04it/s]\u001b[A\n","Iteration:  53%|█████▎    | 19/36 [00:18<00:16,  1.04it/s]\u001b[A\n","Iteration:  56%|█████▌    | 20/36 [00:19<00:15,  1.04it/s]\u001b[A\n","Iteration:  58%|█████▊    | 21/36 [00:20<00:14,  1.04it/s]\u001b[A\n","Iteration:  61%|██████    | 22/36 [00:21<00:13,  1.04it/s]\u001b[A\n","Iteration:  64%|██████▍   | 23/36 [00:22<00:12,  1.04it/s]\u001b[A\n","Iteration:  67%|██████▋   | 24/36 [00:23<00:11,  1.03it/s]\u001b[A\n","Iteration:  69%|██████▉   | 25/36 [00:24<00:10,  1.03it/s]\u001b[A\n","Iteration:  72%|███████▏  | 26/36 [00:25<00:09,  1.03it/s]\u001b[A\n","Iteration:  75%|███████▌  | 27/36 [00:26<00:08,  1.03it/s]\u001b[A\n","Iteration:  78%|███████▊  | 28/36 [00:27<00:07,  1.03it/s]\u001b[A\n","Iteration:  81%|████████  | 29/36 [00:28<00:06,  1.03it/s]\u001b[A\n","Iteration:  83%|████████▎ | 30/36 [00:29<00:05,  1.03it/s]\u001b[A\n","Iteration:  86%|████████▌ | 31/36 [00:30<00:04,  1.03it/s]\u001b[A\n","Iteration:  89%|████████▉ | 32/36 [00:31<00:03,  1.03it/s]\u001b[A\n","Iteration:  92%|█████████▏| 33/36 [00:31<00:02,  1.03it/s]\u001b[A\n","Iteration:  94%|█████████▍| 34/36 [00:32<00:01,  1.03it/s]\u001b[A\n","Iteration:  97%|█████████▋| 35/36 [00:33<00:00,  1.03it/s]\u001b[A\n","Iteration: 100%|██████████| 36/36 [00:34<00:00,  1.05it/s]\n","Epoch: 100%|██████████| 6/6 [04:35<00:00, 45.93s/it]"],"name":"stderr"},{"output_type":"stream","text":["[epoch 6] loss: 5.700, acc: 0.994\n","    >> 訓練完成，儲存訓練完之模型\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"9i8n7_zCdwl1"},"source":[""]},{"cell_type":"code","metadata":{"id":"0tHN1XqvdhgD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L6mM64_zJmO1"},"source":[""],"execution_count":null,"outputs":[]}]}